{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x1ypzczQCwy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using TensorFlow Recommenders with TFX\n",
    "\n",
    "***A tutorial to train a TensorFlow Recommenders ranking model as a [TFX pipeline](https://www.tensorflow.org/tfx).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HU9YYythm0dx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/ranking_tfx\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/ranking_tfx.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VuwrlnvQJ5k",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook-based tutorial, we will create and run a [TFX pipeline](https://www.tensorflow.org/tfx)\n",
    "to train a ranking model to predict movie ratings using TensorFlow Recommenders (TFRS).\n",
    "The pipeline will consist of three essential TFX components: ExampleGen,\n",
    "Trainer and Pusher. The pipeline includes the most minimal ML workflow like\n",
    "importing data, training a model and exporting the trained TFRS ranking model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDnPgN8UJtzN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the TensorFlow and TFX versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jh7vKSRqPHb",
    "outputId": "d316e143-fc86-493e-ec2e-43181bee8e7c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 18:47:49.061999: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:47:49.062026: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.2\n",
      "TFX version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "There are some variables used to define a pipeline. You can customize these\n",
    "variables as you want. By default all output from the pipeline will be\n",
    "generated under the current directory. Instead of using the SchemaGen component to generate a schema, for this\n",
    "tutorial we will create a hardcoded schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DCN-iterate\n",
      "pipeline/pipelines/DCN-iterate\n",
      "pipeline/metadata/DCN-iterate/metadata.db\n",
      "pipeline/serving_model/DCN-iterate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "PIPELINE_NAME = 'DCN-iterate'\n",
    "WORKING_DIR = 'bachelor_2022/artifact'\n",
    "PIPE_DIR = 'pipeline'\n",
    "\n",
    "# Directory where MovieLens 100K rating data lives\n",
    "DATA_ROOT = os.path.join('data', PIPELINE_NAME)\n",
    "print(DATA_ROOT)\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join(PIPE_DIR, 'pipelines', PIPELINE_NAME)\n",
    "print(PIPELINE_ROOT)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join(PIPE_DIR, 'metadata', PIPELINE_NAME, 'metadata.db')\n",
    "print(METADATA_PATH)\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join(PIPE_DIR, 'serving_model', PIPELINE_NAME)\n",
    "print(SERVING_MODEL_DIR)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DCN-iterate\n"
     ]
    }
   ],
   "source": [
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASpoNmxKSQjI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eSz28UDSnlG",
    "outputId": "d00e061d-e2fc-4358-c7fd-3e00e5bbc684",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'data/DCN-iterate/ml-100k_colab.csv' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head {DATA_ROOT}/ml-100k_colab.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTtQNq1DdVvG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should be able to see four values. For example, the first example means user '196' gives a rating of 3 to movie '242'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH6gizcpSwWV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a pipeline\n",
    "\n",
    "TFX pipelines are defined using Python APIs. We will define a pipeline which\n",
    "consists of following three components.\n",
    "- CsvExampleGen: Reads in data files and convert them to TFX internal format\n",
    "for further processing. There are multiple\n",
    "[ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)s for various\n",
    "formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
    "- Trainer: Trains an ML model.\n",
    "[Trainer component](https://www.tensorflow.org/tfx/guide/trainer) requires a\n",
    "model definition code from users. You can use TensorFlow APIs to specify how to\n",
    "train a model and save it in a _saved_model_ format.\n",
    "- Pusher: Copies the trained model outside of the TFX pipeline.\n",
    "[Pusher component](https://www.tensorflow.org/tfx/guide/pusher) can be thought\n",
    "of an deployment process of the trained ML model.\n",
    "\n",
    "Before actually define the pipeline, we need to write a model code for the\n",
    "Trainer component first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjDv93eS5xV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write model training code\n",
    "\n",
    "We will build a simple ranking model to predict movie ratings. This model training code will be saved to a separate file.\n",
    "\n",
    "In this tutorial we will use\n",
    "[Generic Trainer](https://www.tensorflow.org/tfx/guide/trainer#generic_trainer)\n",
    "of TFX which support Keras-based models. You need to write a Python file\n",
    "containing `run_fn` function, which is the entrypoint for the `Trainer`\n",
    "component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aES7Hv5QTDK3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'dcn_ranking_training.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFsQCOytiidq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ranking model we use is almost exactly the same as in the [Basic Ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking) tutorial. The only difference is that we use movie IDs instead of movie titles in the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gnc67uQNTDfW",
    "outputId": "06f4ccb2-d0f5-4cae-e4f8-98317f7d29c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dcn_ranking_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import Dict, Text\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "_FEATURE_KEYS = [\"movie_id\",\"user_id\",\"user_gender\", \"user_occupation\", \"user_age_cohort\"]\n",
    "_LABEL_KEY = 'user_rating'\n",
    "\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # Define the dimension the features should be embedded in (Dim of vector representation of each feature)\n",
    "    embedding_dimension = 32\n",
    "\n",
    "    # Create np array with incrementing values as the vocabulary\n",
    "    unique_user_ids = np.array(range(943)).astype(str)\n",
    "    unique_movie_ids = np.array(range(1682)).astype(str)\n",
    "    unique_occupation_ids = np.array(range(21)).astype(str)\n",
    "    unique_gender_ids = np.array(range(2)).astype(str)\n",
    "    unique_age_ids = np.array(range(7)).astype(str)\n",
    "\n",
    "\n",
    "    ## String values embeddings\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_id', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        # Input of 943 dims -->  Embedding of 32 dims\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='movie_id', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_movie_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_movie_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for occupations.\n",
    "    self.occupation_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_occupation', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_occupation_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_occupation_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    ## Int value embeddings\n",
    "    # Compute embeddings for gender.\n",
    "    self.gender_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_gender', dtype=tf.int64),\n",
    "        tf.keras.layers.IntegerLookup(\n",
    "            vocabulary=unique_gender_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_gender_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for age.\n",
    "    self.age_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_age_cohort', dtype=tf.int64),\n",
    "        tf.keras.layers.IntegerLookup(\n",
    "            vocabulary=unique_age_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_age_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "        tfrs.layers.dcn.Cross(),\n",
    "        tfrs.layers.dcn.Cross(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, movie_id, user_gender, user_occupation, user_age = inputs\n",
    "\n",
    "    user_embedding = self.user_embeddings(user_id)\n",
    "    movie_embedding = self.movie_embeddings(movie_id)\n",
    "    gender_embedding = self.gender_embeddings(user_gender)\n",
    "    occupation_embedding = self.occupation_embeddings(user_occupation)\n",
    "    age_embedding = self.age_embeddings(user_age)\n",
    "\n",
    "\n",
    "\n",
    "    return self.ratings(tf.concat([user_embedding, movie_embedding, gender_embedding, occupation_embedding, age_embedding], axis=2))\n",
    "\n",
    "\n",
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = RankingModel()\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model((features['user_id'], features['movie_id'], features['user_gender'], features['user_occupation'], features['user_age_cohort']))\n",
    "\n",
    "  def compute_loss(self,\n",
    "                   features: Dict[Text, tf.Tensor],\n",
    "                   training=False) -> tf.Tensor:\n",
    "\n",
    "    labels = features[1]\n",
    "    rating_predictions = self(features[0])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(labels=labels, predictions=rating_predictions)\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 256) -> tf.data.Dataset:\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  return MovielensModel()\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  from datetime import datetime\n",
    "  logdir = \"pipeline/pipelines/DCN-iterate/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files, fn_args.data_accessor, schema, batch_size=8192)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, fn_args.data_accessor, schema, batch_size=4096)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      epochs = 3,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "  model.save(fn_args.serving_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline/pipelines/DCN-iterate\n"
     ]
    }
   ],
   "source": [
    "print(PIPELINE_ROOT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blaw0rs-emEf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you have completed all preparation steps to build the TFX pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OkNz3gTLwM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write a pipeline definition\n",
    "\n",
    "We define a function to create a TFX pipeline. A `Pipeline` object\n",
    "represents a TFX pipeline which can be run using one of pipeline\n",
    "orchestration systems that TFX supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M49yYVNBTPd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=12),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=24))\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      trainer,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJbq07THU2GV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "TFX supports multiple orchestrators to run pipelines.\n",
    "In this tutorial we will use `LocalDagRunner` which is included in the TFX\n",
    "Python package and runs pipelines on local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mp0AkmrPdUb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we create a `LocalDagRunner` and pass a `Pipeline` object created from the\n",
    "function we already defined.\n",
    "\n",
    "The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "fAtfOZTYWJu-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "53fa7d90-a1fc-4704-ed2a-45f56488fa0b",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating ephemeral wheel package for '/home/cory/PycharmProjects/bachelor_2022/artifact/dcn_ranking_training.py' (including modules: ['dcn_ranking_training', 'tfrs_ranking_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.\n",
      "INFO:absl:Executing: ['/home/cory/anaconda3/envs/bachelor_2022/bin/python', '/tmp/tmprdgpz6e_/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpvyl2ej9f', '--dist-dir', '/tmp/tmpf83xu5d8']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying dcn_ranking_training.py -> build/lib\n",
      "copying tfrs_ranking_trainer.py -> build/lib\n",
      "installing to /tmp/tmpvyl2ej9f\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/tfrs_ranking_trainer.py -> /tmp/tmpvyl2ej9f\n",
      "copying build/lib/dcn_ranking_training.py -> /tmp/tmpvyl2ej9f\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cory/anaconda3/envs/bachelor_2022/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "INFO:absl:Successfully built user code wheel distribution at 'pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl'; target user module is 'dcn_ranking_training'.\n",
      "INFO:absl:Full user module path is 'dcn_ranking_training@pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"pipeline/metadata/DCN-iterate/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"pipeline/metadata/DCN-iterate/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data/DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpvyl2ej9f/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3.8.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpvyl2ej9f/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.dist-info/WHEEL\n",
      "creating '/tmp/tmpf83xu5d8/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl' and adding '/tmp/tmpvyl2ej9f' to it\n",
      "adding 'dcn_ranking_training.py'\n",
      "adding 'tfrs_ranking_trainer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1.dist-info/RECORD'\n",
      "removing /tmp/tmpvyl2ej9f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 13\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=13, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/CsvExampleGen/examples/13\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_base': 'data/DCN-iterate', 'output_data_format': 6, 'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009'}, execution_output_uri='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/executor_execution/13/executor_output.pb', stateful_working_dir='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/stateful_working_dir/2022-07-18T18:47:56.299054', tmp_dir='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/executor_execution/13/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data/DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"DCN-iterate\"\n",
      ", pipeline_run_id='2022-07-18T18:47:56.299054')\n",
      "INFO:absl:Generating examples.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processing input csv data data/DCN-iterate/* to TFExample.\n",
      "E0718 18:47:57.224307258    3085 fork_posix.cc:76]           Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 13 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/CsvExampleGen/examples/13\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 13\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-18T18:47:56.299054\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 24\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"dcn_ranking_training@pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 12\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 14\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=14, input_dict={'examples': [Artifact(artifact: id: 17\n",
      "type_id: 15\n",
      "uri: \"pipeline/pipelines/DCN-iterate/CsvExampleGen/examples/13\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:CsvExampleGen:examples:0\"\n",
      "create_time_since_epoch: 1658162904280\n",
      "last_update_time_since_epoch: 1658162904280\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Trainer/model_run/14\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model_run:0\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Trainer/model/14\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 24\\n}', 'module_path': 'dcn_ranking_training@pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 12\\n}'}, execution_output_uri='pipeline/pipelines/DCN-iterate/Trainer/.system/executor_execution/14/executor_output.pb', stateful_working_dir='pipeline/pipelines/DCN-iterate/Trainer/.system/stateful_working_dir/2022-07-18T18:47:56.299054', tmp_dir='pipeline/pipelines/DCN-iterate/Trainer/.system/executor_execution/14/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-18T18:47:56.299054\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 24\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"dcn_ranking_training@pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 12\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"DCN-iterate\"\n",
      ", pipeline_run_id='2022-07-18T18:47:56.299054')\n",
      "INFO:absl:Train on the 'train' split when train_args.splits is not set.\n",
      "INFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\n",
      "INFO:absl:udf_utils.get_fn {'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 24\\n}', 'module_path': 'dcn_ranking_training@pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 12\\n}'} 'run_fn'\n",
      "INFO:absl:Installing 'pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['/home/cory/anaconda3/envs/bachelor_2022/bin/python', '-m', 'pip', 'install', '--target', '/tmp/tmpr2p86zdq', 'pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Successfully installed 'pipeline/pipelines/DCN-iterate/_wheels/tfx_user_code_Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n",
      "INFO:absl:Feature movie_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_age_cohort has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_occupation has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_rating has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tfx-user-code-Trainer\n",
      "Successfully installed tfx-user-code-Trainer-0.0+f4763b4c8f134548a8236ece7f6af206f19156137d5b0a2a74f9d32415e484f1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 18:48:26.239821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-18 18:48:26.240044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240255: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240306: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240357: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-18 18:48:26.240421: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "INFO:absl:Feature movie_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_age_cohort has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_occupation has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_rating has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature movie_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_age_cohort has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_occupation has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_rating has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature movie_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_age_cohort has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_id has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_occupation has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature user_rating has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cory/anaconda3/envs/bachelor_2022/lib/python3.8/site-packages/numpy/core/numeric.py:2449: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 3s 199ms/step - root_mean_squared_error: 2.1042 - loss: 4.1791 - regularization_loss: 0.0000e+00 - total_loss: 4.1791 - val_root_mean_squared_error: 1.1241 - val_loss: 1.4113 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.4113\n",
      "Epoch 2/3\n",
      "12/12 [==============================] - 2s 157ms/step - root_mean_squared_error: 1.1612 - loss: 1.3636 - regularization_loss: 0.0000e+00 - total_loss: 1.3636 - val_root_mean_squared_error: 1.2550 - val_loss: 1.5341 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.5341\n",
      "Epoch 3/3\n",
      "12/12 [==============================] - 2s 166ms/step - root_mean_squared_error: 1.1147 - loss: 1.2376 - regularization_loss: 0.0000e+00 - total_loss: 1.2376 - val_root_mean_squared_error: 1.1093 - val_loss: 1.3268 - val_regularization_loss: 0.0000e+00 - val_total_loss: 1.3268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 18:48:35.444176: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as ranking_layer_call_fn, ranking_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipeline/pipelines/DCN-iterate/Trainer/model/14/Format-Serving/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipeline/pipelines/DCN-iterate/Trainer/model/14/Format-Serving/assets\n",
      "INFO:absl:Training complete. Model written to pipeline/pipelines/DCN-iterate/Trainer/model/14/Format-Serving. ModelRun written to pipeline/pipelines/DCN-iterate/Trainer/model_run/14\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 14 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Trainer/model_run/14\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model_run:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model_run:0\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Trainer/model/14\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 14\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Trainer is finished.\n",
      "INFO:absl:Component Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-18T18:47:56.299054\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"pipeline/serving_model/DCN-iterate\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 15\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=15, input_dict={'model': [Artifact(artifact: id: 19\n",
      "type_id: 18\n",
      "uri: \"pipeline/pipelines/DCN-iterate/Trainer/model/14\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Trainer:model:0\"\n",
      "create_time_since_epoch: 1658162917407\n",
      "last_update_time_since_epoch: 1658162917407\n",
      ", artifact_type: id: 18\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Pusher/pushed_model/15\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Pusher:pushed_model:0\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"pipeline/serving_model/DCN-iterate\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='pipeline/pipelines/DCN-iterate/Pusher/.system/executor_execution/15/executor_output.pb', stateful_working_dir='pipeline/pipelines/DCN-iterate/Pusher/.system/stateful_working_dir/2022-07-18T18:47:56.299054', tmp_dir='pipeline/pipelines/DCN-iterate/Pusher/.system/executor_execution/15/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-18T18:47:56.299054\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-07-18T18:47:56.299054\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"DCN-iterate.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"pipeline/serving_model/DCN-iterate\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"DCN-iterate\"\n",
      ", pipeline_run_id='2022-07-18T18:47:56.299054')\n",
      "WARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\n",
      "INFO:absl:Model version: 1658162917\n",
      "INFO:absl:Model written to serving path pipeline/serving_model/DCN-iterate/1658162917.\n",
      "INFO:absl:Model pushed to pipeline/pipelines/DCN-iterate/Pusher/pushed_model/15.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 15 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/Pusher/pushed_model/15\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-18T18:47:56.299054:Pusher:pushed_model:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.8.0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-18T18:47:56.299054:Pusher:pushed_model:0\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 15\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppERq0Mj6xvW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should see \"INFO:absl:Component Pusher is finished.\" at the end of the\n",
    "logs if the pipeline finished successfully. Because `Pusher` component is the\n",
    "last component of the pipeline.\n",
    "\n",
    "The pusher component pushes the trained model to the `SERVING_MODEL_DIR` which\n",
    "is the `serving_model/TFRS-ranking` directory if you did not change the\n",
    "variables in the previous steps. You can see the result from the file browser\n",
    "in the left-side panel in Colab, or using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTHROkqX6yHx",
    "outputId": "0f0ec5bd-9977-4a94-aa7c-72a793d3f819",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline/serving_model/DCN-iterate:\r\n",
      "1658143931  1658156250\t1658156418  1658156805\t1658162917\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n"
     ]
    }
   ],
   "source": [
    "# List files in created model directory.\n",
    "!ls -R {SERVING_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8HQfT-ziids",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can test the ranking model by computing predictions for a user and a movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EDMkz8Wiidt",
    "outputId": "7d4b8506-18b2-4d40-b58e-2744c67f6940",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.4894285]]]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Load the latest model for testing\n",
    "loaded = tf.saved_model.load(max(glob.glob(os.path.join(SERVING_MODEL_DIR, '*/')), key=os.path.getmtime))\n",
    "print(loaded({'user_id': [[42]], 'movie_id': [[15]], 'user_gender': [[1]], 'user_occupation': [[0]], 'user_age_cohort': [[0]]}).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08R8qvweThRf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This concludes the TensorFlow Recommenders + TFX tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DjUA6S30k52h"
   ],
   "name": "ranking_tfx.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}