{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x1ypzczQCwy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DCN with TFX\n",
    "\n",
    "***DCN TensorFlow Recommenders ranking model as a\n",
    "References:***\n",
    "[TFX pipeline](https://www.tensorflow.org/recommenders/examples/ranking_tfx).\n",
    "[DCN](https://www.tensorflow.org/recommenders/examples/dcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDnPgN8UJtzN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check the TensorFlow and TFX versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jh7vKSRqPHb",
    "outputId": "d316e143-fc86-493e-ec2e-43181bee8e7c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-23 23:57:51.257227: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \n",
      "2022-07-23 23:57:51.257254: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.1\n",
      "TFX version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDtLdSkvqPHe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EcUseqJaE2XN",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/cory/PycharmProjects/bachelor_2022/artifact/pipeline_scripts\n",
      "/home/cory/PycharmProjects/bachelor_2022/artifact\n",
      "data/DCN-iterate\n",
      "pipeline/pipelines/DCN-iterate\n",
      "pipeline/metadata/DCN-iterate/metadata.db\n",
      "pipeline/serving_model/DCN-iterate\n",
      "pipeline/pipelines/DCN-iterate/plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "\n",
    "PIPELINE_NAME = 'DCN-iterate'\n",
    "WORKING_DIR = 'bachelor_2022/artifact'\n",
    "PIPE_DIR = 'pipeline'\n",
    "\n",
    "# Directory where MovieLens 100K rating data resides\n",
    "DATA_ROOT = os.path.join('data', PIPELINE_NAME)\n",
    "print(DATA_ROOT)\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join(PIPE_DIR, 'pipelines', PIPELINE_NAME)\n",
    "print(PIPELINE_ROOT)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join(PIPE_DIR, 'metadata', PIPELINE_NAME, 'metadata.db')\n",
    "print(METADATA_PATH)\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join(PIPE_DIR, 'serving_model', PIPELINE_NAME)\n",
    "print(SERVING_MODEL_DIR)\n",
    "\n",
    "MODEL_PLOTS = os.path.join(PIPE_DIR, 'pipelines', PIPELINE_NAME, 'plots')\n",
    "print(MODEL_PLOTS)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/DCN-iterate\n"
     ]
    }
   ],
   "source": [
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASpoNmxKSQjI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-eSz28UDSnlG",
    "outputId": "d00e061d-e2fc-4358-c7fd-3e00e5bbc684",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,movie_id,user_rating,timestamp,user_occupation,user_age_cohort,user_gender\r\n",
      "196,242,3,881250949,1,1,1\r\n",
      "305,242,5,886307828,2,2,1\r\n",
      "6,242,4,883268170,3,3,1\r\n",
      "234,242,4,891033261,4,4,1\r\n",
      "63,242,3,875747190,5,5,1\r\n",
      "181,242,1,878961814,3,5,1\r\n",
      "201,242,4,884110598,1,5,1\r\n",
      "249,242,5,879571438,6,5,1\r\n",
      "13,242,2,881515193,7,1,1\r\n"
     ]
    }
   ],
   "source": [
    "!head {DATA_ROOT}/ml-100k_ready.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTtQNq1DdVvG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You should be able to see four values. For example, the first example means user '196' gives a rating of 3 to movie '242'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH6gizcpSwWV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a pipeline\n",
    "\n",
    "TFX pipelines are defined using Python APIs. We will define a pipeline which\n",
    "consists of following three components.\n",
    "- CsvExampleGen: Reads in data files and convert them to TFX internal format\n",
    "for further processing. There are multiple\n",
    "[ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)s for various\n",
    "formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
    "- Trainer: Trains an ML model.\n",
    "[Trainer component](https://www.tensorflow.org/tfx/guide/trainer) requires a\n",
    "model definition code from users. You can use TensorFlow APIs to specify how to\n",
    "train a model and save it in a _saved_model_ format.\n",
    "- Pusher: Copies the trained model outside of the TFX pipeline.\n",
    "[Pusher component](https://www.tensorflow.org/tfx/guide/pusher) can be thought\n",
    "of an deployment process of the trained ML model.\n",
    "\n",
    "Before actually define the pipeline, we need to write a model code for the\n",
    "Trainer component first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOjDv93eS5xV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write model training code\n",
    "\n",
    "We will build a simple ranking model to predict movie ratings. This model training code will be saved to a separate file.\n",
    "\n",
    "In this tutorial we will use\n",
    "[Generic Trainer](https://www.tensorflow.org/tfx/guide/trainer#generic_trainer)\n",
    "of TFX which support Keras-based models. You need to write a Python file\n",
    "containing `run_fn` function, which is the entrypoint for the `Trainer`\n",
    "component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aES7Hv5QTDK3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'model_source/dcn_ranking_training.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFsQCOytiidq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ranking model we use is almost exactly the same as in the [Basic Ranking](https://www.tensorflow.org/recommenders/examples/basic_ranking) tutorial. The only difference is that we use movie IDs instead of movie titles in the candidate tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gnc67uQNTDfW",
    "outputId": "06f4ccb2-d0f5-4cae-e4f8-98317f7d29c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model_source/dcn_ranking_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from typing import Dict, Text\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_recommenders as tfrs\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "_FEATURE_KEYS = [\"movie_id\",\"user_id\",\"user_gender\", \"user_occupation\", \"user_age_cohort\"]\n",
    "_LABEL_KEY = 'user_rating'\n",
    "\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "        for feature in _FEATURE_KEYS\n",
    "    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "class RankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # Define the dimension the features should be embedded in (Dim of vector representation of each feature)\n",
    "    embedding_dimension = 32\n",
    "    self.embedding_dims = embedding_dimension\n",
    "    # Create np array with incrementing values as the vocabulary\n",
    "    unique_user_ids = np.array(range(943)).astype(str)\n",
    "    unique_movie_ids = np.array(range(1682)).astype(str)\n",
    "    unique_occupation_ids = np.array(range(21)).astype(str)\n",
    "    unique_gender_ids = np.array(range(2)).astype(str)\n",
    "    unique_age_ids = np.array(range(7)).astype(str)\n",
    "\n",
    "\n",
    "    ## String values embeddings\n",
    "    # Compute embeddings for users.\n",
    "    self.user_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_id', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_user_ids, mask_token=None),\n",
    "        # Input of 943 dims -->  Embedding of 32 dims\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_user_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for movies.\n",
    "    self.movie_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='movie_id', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_movie_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_movie_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for occupations.\n",
    "    self.occupation_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_occupation', dtype=tf.int64),\n",
    "        tf.keras.layers.Lambda(lambda x: tf.as_string(x)),\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=unique_occupation_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_occupation_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    ## Int value embeddings\n",
    "    # Compute embeddings for gender.\n",
    "    self.gender_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_gender', dtype=tf.int64),\n",
    "        tf.keras.layers.IntegerLookup(\n",
    "            vocabulary=unique_gender_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_gender_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Compute embeddings for age.\n",
    "    self.age_embeddings = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(1,), name='user_age_cohort', dtype=tf.int64),\n",
    "        tf.keras.layers.IntegerLookup(\n",
    "            vocabulary=unique_age_ids, mask_token=None),\n",
    "        tf.keras.layers.Embedding(\n",
    "            len(unique_age_ids) + 1, embedding_dimension)\n",
    "    ])\n",
    "\n",
    "    # Cross Layer\n",
    "    self.cross_layer = tfrs.layers.dcn.Cross(kernel_initializer=tf.keras.initializers.RandomNormal(seed=1)) # Use seeds to make model reproducible\n",
    "\n",
    "    # Compute predictions.\n",
    "    self.ratings = tf.keras.Sequential([\n",
    "        self.cross_layer,\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(seed=1)),\n",
    "        tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.RandomNormal(seed=1)),\n",
    "        tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.RandomNormal(seed=1))\n",
    "    ])\n",
    "\n",
    "  def call(self, inputs):\n",
    "\n",
    "    user_id, movie_id, user_gender, user_occupation, user_age = inputs\n",
    "\n",
    "    # Calculate embedding for each feature and save in *_embedding variable\n",
    "    user_embedding = self.user_embeddings(user_id)\n",
    "    movie_embedding = self.movie_embeddings(movie_id)\n",
    "    gender_embedding = self.gender_embeddings(user_gender)\n",
    "    occupation_embedding = self.occupation_embeddings(user_occupation)\n",
    "    age_embedding = self.age_embeddings(user_age)\n",
    "\n",
    "\n",
    "    # Create embedding layer\n",
    "    return self.ratings(tf.concat([user_embedding, movie_embedding, gender_embedding, occupation_embedding, age_embedding], axis=2))\n",
    "\n",
    "\n",
    "class MovielensModel(tfrs.models.Model):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.ranking_model: tf.keras.Model = RankingModel()\n",
    "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    return self.ranking_model((features['user_id'], features['movie_id'], features['user_gender'], features['user_occupation'], features['user_age_cohort']))\n",
    "\n",
    "  def compute_loss(self,\n",
    "                   features: Dict[Text, tf.Tensor],\n",
    "                   training=False) -> tf.Tensor:\n",
    "\n",
    "    labels = features[1]\n",
    "    rating_predictions = self(features[0])\n",
    "\n",
    "    # The task computes the loss and the metrics.\n",
    "    return self.task(labels=labels, predictions=rating_predictions)\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 256) -> tf.data.Dataset:\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  return MovielensModel()\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # Generate training logfiles for tensorboard\n",
    "  from datetime import datetime\n",
    "  logdir = \"pipeline/pipelines/DCN-iterate/logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "  # Derive data schema from generated _FEATURE_SPEC dictionary\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files, fn_args.data_accessor, schema, batch_size=8192)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files, fn_args.data_accessor, schema, batch_size=4096)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
    "\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      epochs = 1,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "  model.save(fn_args.serving_model_dir)\n",
    "\n",
    "\n",
    "  ###  Display model summary\n",
    "  print(\"\\n#####################################\")\n",
    "  print(model.summary())\n",
    "  print()\n",
    "\n",
    "  # Save plot of model architecture\n",
    "  model_num = fn_args.serving_model_dir.split(\"/\")[-2]   # extract model number\n",
    "  img_dir = fn_args.custom_config[\"plot_path\"] + f\"/{model_num}\"\n",
    "  print(img_dir)\n",
    "  Path(img_dir).mkdir(parents=True, exist_ok=True)\n",
    "  tf.keras.utils.plot_model(model.ranking_model.ratings, to_file=f\"{img_dir}/model_architecture_{model_num}.png\", show_shapes=True)\n",
    "  print()\n",
    "\n",
    "  ### Cross feature Visualization\n",
    "  mat = model.ranking_model.cross_layer._dense.kernel # Cross weights matrix\n",
    "  features = _FEATURE_KEYS\n",
    "\n",
    "  block_norm = np.ones([len(features), len(features)])\n",
    "  dim = model.ranking_model.embedding_dims\n",
    "\n",
    "  # Compute the norms of the blocks.\n",
    "  for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "      # Norm of 32x32 Matrix is calculated | 32x32 values --> 1 value\n",
    "      block = mat[i * dim:(i + 1) * dim,    # 32x32 blocks are retrieved from cross network\n",
    "                  j * dim:(j + 1) * dim]\n",
    "      block_norm[i,j] = np.linalg.norm(block, ord=\"fro\") # Frobenius norm is used | norm of each matrix element is calculated and added together\n",
    "  # Create plot\n",
    "  plt.figure(figsize=(9,9))\n",
    "  im = plt.matshow(block_norm, cmap=plt.cm.Blues)\n",
    "  ax = plt.gca()\n",
    "  divider = make_axes_locatable(plt.gca())\n",
    "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "  plt.colorbar(im, cax=cax)\n",
    "  cax.tick_params(labelsize=10)\n",
    "  _ = ax.set_xticklabels([\"\"] + features, rotation=45, ha=\"left\", fontsize=10)\n",
    "  _ = ax.set_yticklabels([\"\"] + features, fontsize=10)\n",
    "\n",
    "  plt.savefig(f\"{img_dir}/cross_features_{model_num}\", dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blaw0rs-emEf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now you have completed all preparation steps to build the TFX pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3OkNz3gTLwM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write a pipeline definition\n",
    "\n",
    "We define a function to create a TFX pipeline. A `Pipeline` object\n",
    "represents a TFX pipeline which can be run using one of pipeline\n",
    "orchestration systems that TFX supports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M49yYVNBTPd4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "from google.protobuf import text_format\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str, plot_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # Evaluate rudimentary statistics on the dataset\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples']\n",
    "      )\n",
    "\n",
    "  # Read and infer schema of the dataset\n",
    "  schema_gen = tfx.components.SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics']\n",
    "  )\n",
    "\n",
    "  # Check dataset for anomalies\n",
    "  example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=12),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=24),\n",
    "      custom_config={\"plot_path\": plot_path})\n",
    "\n",
    "\n",
    "  # Evaluation of trained model against deployed models\n",
    "\n",
    "  model_resolver = tfx.dsl.Resolver(\n",
    "        strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "        model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "        model_blessing=tfx.dsl.Channel(\n",
    "            type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "                'latest_blessed_model_resolver')\n",
    "\n",
    "  eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key=\"user_rating\")],\n",
    "      slicing_specs=[tfma.SlicingSpec()],\n",
    "      metrics_specs = [\n",
    "          tfma.MetricsSpec(metrics=[\n",
    "              tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
    "              tfma.MetricConfig(class_name=\"MeanSquaredError\"),\n",
    "              tfma.MetricConfig(class_name=\"BinaryAccuracy\",\n",
    "                   threshold=tfma.MetricThreshold(\n",
    "                       value_threshold=tfma.GenericValueThreshold(\n",
    "                           lower_bound={\"value\":0.1}),\n",
    "                       change_threshold=tfma.GenericChangeThreshold(\n",
    "                           direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                           absolute={\"value\":0.001})\n",
    "                   ))\n",
    "\n",
    "          ])\n",
    "          ]\n",
    "      )\n",
    "\n",
    "  evaluator = tfx.components.Evaluator(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      baseline_model=model_resolver.outputs['model'],\n",
    "      eval_config=eval_config)\n",
    "\n",
    "  # Pushes the model to a filesystem destination.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  # Following three components will be included in the pipeline.\n",
    "  components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      schema_gen,\n",
    "      example_validator,\n",
    "      trainer,\n",
    "      # model_resolver,\n",
    "      # evaluator,\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJbq07THU2GV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run the pipeline\n",
    "\n",
    "TFX supports multiple orchestrators to run pipelines.\n",
    "In this tutorial we will use `LocalDagRunner` which is included in the TFX\n",
    "Python package and runs pipelines on local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mp0AkmrPdUb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we create a `LocalDagRunner` and pass a `Pipeline` object created from the\n",
    "function we already defined.\n",
    "\n",
    "The pipeline runs directly and you can see logs for the progress of the pipeline including ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner\n",
    "#\n",
    "# AirflowDagRunner().run(\n",
    "#   _create_pipeline(\n",
    "#       pipeline_name=PIPELINE_NAME,\n",
    "#       pipeline_root=PIPELINE_ROOT,\n",
    "#       data_root=DATA_ROOT,\n",
    "#       module_file=_trainer_module_file,\n",
    "#       serving_model_dir=SERVING_MODEL_DIR,\n",
    "#       metadata_path=METADATA_PATH,\n",
    "#       plot_path=MODEL_PLOTS))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "fAtfOZTYWJu-",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "53fa7d90-a1fc-4704-ed2a-45f56488fa0b",
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"pipeline/metadata/DCN-iterate/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"pipeline/metadata/DCN-iterate/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-24T00:14:00.834729\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data/DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 248\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=248, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/CsvExampleGen/examples/248\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-24T00:14:00.834729:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-24T00:14:00.834729:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_data_format': 6, 'input_base': 'data/DCN-iterate', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009'}, execution_output_uri='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/executor_execution/248/executor_output.pb', stateful_working_dir='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/stateful_working_dir/2022-07-24T00:14:00.834729', tmp_dir='pipeline/pipelines/DCN-iterate/CsvExampleGen/.system/executor_execution/248/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-07-24T00:14:00.834729\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"DCN-iterate.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data/DCN-iterate\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"DCN-iterate\"\n",
      ", pipeline_run_id='2022-07-24T00:14:00.834729')\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data data/DCN-iterate/* to TFExample.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 248 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipeline/pipelines/DCN-iterate/CsvExampleGen/examples/248\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:2617288,xor_checksum:1658143009,sum_checksum:1658143009\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"DCN-iterate:2022-07-24T00:14:00.834729:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.9.0\"\n",
      "  }\n",
      "}\n",
      "name: \"DCN-iterate:2022-07-24T00:14:00.834729:CsvExampleGen:examples:0\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 248\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_pipeline(\n",
    "      pipeline_name=PIPELINE_NAME,\n",
    "      pipeline_root=PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir=SERVING_MODEL_DIR,\n",
    "      metadata_path=METADATA_PATH,\n",
    "      plot_path=MODEL_PLOTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NTHROkqX6yHx",
    "outputId": "0f0ec5bd-9977-4a94-aa7c-72a793d3f819",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline/serving_model/DCN-iterate:\r\n",
      "1658143931  1658241808\t1658247911  1658265853\t1658272757  1658432217\r\n",
      "1658156250  1658242623\t1658261251  1658266121\t1658275631  1658440682\r\n",
      "1658156418  1658243152\t1658263017  1658266549\t1658276011  1658442704\r\n",
      "1658156805  1658244105\t1658263703  1658270110\t1658276127  1658444170\r\n",
      "1658162917  1658244394\t1658263766  1658270183\t1658278666  1658446396\r\n",
      "1658163244  1658244568\t1658264266  1658270309\t1658358720  1658451581\r\n",
      "1658163413  1658246554\t1658264498  1658270419\t1658427634  1658613537\r\n",
      "1658241672  1658247040\t1658264743  1658272648\t1658428709\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658143931/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156250/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156418/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658156805/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658162917/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163244:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163244/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163244/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163413:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163413/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658163413/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241672:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241672/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241672/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241808:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241808/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658241808/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658242623:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658242623/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658242623/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658243152:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658243152/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658243152/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244105:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244105/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244105/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244394:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244394/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244394/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244568:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244568/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658244568/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658246554:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658246554/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658246554/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247040:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247040/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247040/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247911:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247911/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658247911/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658261251:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658261251/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658261251/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263017:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263017/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263017/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263703:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263703/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263703/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263766:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263766/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658263766/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264266:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264266/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264266/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264498:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264498/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264498/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264743:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264743/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658264743/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658265853:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658265853/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658265853/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266121:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266121/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266121/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266549:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266549/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658266549/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270110:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270110/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270110/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270183:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270183/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270183/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270309:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270309/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270309/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270419:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270419/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658270419/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272648:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272648/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272648/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272757:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272757/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658272757/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658275631:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658275631/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658275631/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276011:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276011/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276011/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276127:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276127/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658276127/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658278666:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658278666/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658278666/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658358720:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658358720/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658358720/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658427634:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658427634/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658427634/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658428709:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658428709/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658428709/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658432217:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658432217/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658432217/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658440682:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658440682/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658440682/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658442704:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658442704/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658442704/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658444170:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658444170/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658444170/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658446396:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658446396/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658446396/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658451581:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658451581/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658451581/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658613537:\r\n",
      "assets\tkeras_metadata.pb  saved_model.pb  variables\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658613537/assets:\r\n",
      "\r\n",
      "pipeline/serving_model/DCN-iterate/1658613537/variables:\r\n",
      "variables.data-00000-of-00001  variables.index\r\n"
     ]
    }
   ],
   "source": [
    "# List files in created model directory.\n",
    "!ls -R {SERVING_MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8HQfT-ziids",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can test the ranking model by computing predictions for a user and a movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5EDMkz8Wiidt",
    "outputId": "7d4b8506-18b2-4d40-b58e-2744c67f6940",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[3.2812827]]]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# Load the latest model for testing\n",
    "loaded = tf.saved_model.load(max(glob.glob(os.path.join(SERVING_MODEL_DIR, '*/')), key=os.path.getmtime))\n",
    "print(loaded({'user_id': [[42]], 'movie_id': [[15]], 'user_gender': [[1]], 'user_occupation': [[0]], 'user_age_cohort': [[0]]}).numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DjUA6S30k52h"
   ],
   "name": "ranking_tfx.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}